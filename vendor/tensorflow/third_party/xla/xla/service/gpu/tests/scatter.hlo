// NOTE: Assertions have been autogenerated by utils/generate-test-checks.py

// RUN: hlo-opt %s --platform=gpu --stage=llvm-before-optimizations --xla_gpu_target_config_filename=%S/../../../tools/hlo_opt/gpu_specs/%{GPU}.txtpb --split-input-file | FileCheck --check-prefixes=CHECK,CHECK-%{PTX} %s

// CHECK-LABEL: entry:
// CHECK:         %[[VAL_0:.*]] = alloca i32, align 4
// CHECK-PTX:     %[[VAL_1:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x
// CHECK-GCN:     %[[VAL_1:.*]] = call i32 @llvm.amdgcn.workgroup.id.x
// CHECK-PTX:     %[[VAL_2:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x
// CHECK-GCN:     %[[VAL_2:.*]] = call i32 @llvm.amdgcn.workitem.id.x
// CHECK:         %[[VAL_3:.*]] = mul nuw nsw i32 %[[VAL_1]], 6
// CHECK:         %[[VAL_4:.*]] = add nuw nsw i32 %[[VAL_3]], %[[VAL_2]]
// CHECK:         %[[VAL_5:.*]] = icmp ult i32 %[[VAL_4]], 6
// CHECK:         call void @llvm.assume(i1 %[[VAL_5]])
// CHECK:         %[[VAL_6:.*]] = add nuw nsw i32 %[[VAL_4]], 0
// CHECK:         %[[VAL_7:.*]] = udiv i32 %[[VAL_6]], 1
// CHECK:         %[[VAL_8:.*]] = urem i32 %[[VAL_7]], 3
// CHECK:         %[[VAL_9:.*]] = udiv i32 %[[VAL_6]], 3
// CHECK:         %[[VAL_10:.*]] = icmp ult i32 %[[VAL_4]], 6
// CHECK:         br i1 %[[VAL_10]], label %[[VAL_11:.*]], label %[[VAL_12:.*]]
// CHECK:       scatter_TensorFlowScatterV1.in_bounds-after:      ; preds = %[[VAL_13:.*]], %[[VAL_14:.*]]
// CHECK:         ret void
// CHECK:       scatter_TensorFlowScatterV1.in_bounds-true:       ; preds = %[[VAL_14]]
// CHECK:         %[[VAL_15:.*]] = getelementptr inbounds [2 x i32], ptr %[[VAL_16:.*]], i32 0, i32 %[[VAL_9]]
// CHECK:         %[[VAL_17:.*]] = load i32, ptr %[[VAL_15]], align 4, !invariant.load
// CHECK:         %[[VAL_18:.*]] = add i32 0, %[[VAL_17]]
// CHECK:         %[[VAL_19:.*]] = icmp ult i32 %[[VAL_17]], 3
// CHECK:         %[[VAL_20:.*]] = and i1 true, %[[VAL_19]]
// CHECK:         br i1 %[[VAL_20]], label %[[VAL_21:.*]], label %[[VAL_13]]
// CHECK:       scatter.in_bounds-after:                          ; preds = %[[VAL_21]], %[[VAL_11]]
// CHECK:         br label %[[VAL_12]]
// CHECK:       scatter.in_bounds-true:                           ; preds = %[[VAL_11]]
// CHECK:         %[[VAL_22:.*]] = getelementptr inbounds [3 x [3 x i32]], ptr %[[VAL_23:.*]], i32 0, i32 %[[VAL_18]], i32 %[[VAL_8]]
// CHECK:         %[[VAL_24:.*]] = getelementptr i32, ptr %[[VAL_25:.*]], i32 %[[VAL_4]]
// CHECK:         %[[VAL_26:.*]] = getelementptr inbounds i32, ptr %[[VAL_24]], i32 0
// CHECK:         %[[VAL_27:.*]] = load i32, ptr %[[VAL_26]], align 4, !invariant.load
// CHECK-PTX:     store i32 %[[VAL_27]], ptr %[[VAL_0]], align 4
// CHECK-GCN:     store i32 %[[VAL_27]], ptr addrspace(5) %[[VAL_0]], align 4
// CHECK-PTX:     %[[VAL_28:.*]] = load i32, ptr %[[VAL_0]], align 4
// CHECK-GCN:     %[[VAL_28:.*]] = load i32, ptr addrspace(5) %[[VAL_0]], align 4
// CHECK:         store atomic i32 %[[VAL_28]], ptr %[[VAL_22]] unordered, align 4
// CHECK:         br label %[[VAL_13]]
// CHECK:       entry:
// CHECK:         %[[VAL_29:.*]] = alloca i32, align 4
// CHECK-PTX:     %[[VAL_30:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x
// CHECK-GCN:     %[[VAL_30:.*]] = call i32 @llvm.amdgcn.workgroup.id.x
// CHECK-PTX:     %[[VAL_31:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x
// CHECK-GCN:     %[[VAL_31:.*]] = call i32 @llvm.amdgcn.workitem.id.x
// CHECK:         %[[VAL_32:.*]] = mul nuw nsw i32 %[[VAL_30]], 1
// CHECK:         %[[VAL_33:.*]] = add nuw nsw i32 %[[VAL_32]], %[[VAL_31]]
// CHECK:         %[[VAL_34:.*]] = icmp ult i32 %[[VAL_33]], 1
// CHECK:         call void @llvm.assume(i1 %[[VAL_34]])
// CHECK:         %[[VAL_35:.*]] = add nuw nsw i32 %[[VAL_33]], 0
// CHECK:         %[[VAL_36:.*]] = icmp ult i32 %[[VAL_33]], 1
// CHECK:         br i1 %[[VAL_36]], label %[[VAL_37:.*]], label %[[VAL_38:.*]]
// CHECK:       scatter_ScatterIntoScalar.in_bounds-after:        ; preds = %[[VAL_39:.*]], %[[VAL_40:.*]]
// CHECK:         ret void
// CHECK:       scatter_ScatterIntoScalar.in_bounds-true:         ; preds = %[[VAL_40]]
// CHECK:         br i1 true, label %[[VAL_41:.*]], label %[[VAL_39]]
// CHECK:       scatter.in_bounds-after:                          ; preds = %[[VAL_41]], %[[VAL_37]]
// CHECK:         br label %[[VAL_38]]
// CHECK:       scatter.in_bounds-true:                           ; preds = %[[VAL_37]]
// CHECK:         %[[VAL_42:.*]] = load i32, ptr %[[VAL_43:.*]], align 4, !invariant.load
// CHECK-PTX:     store i32 %[[VAL_42]], ptr %[[VAL_29]], align 4
// CHECK-GCN:     store i32 %[[VAL_42]], ptr addrspace(5) %[[VAL_29]], align 4
// CHECK-PTX:     %[[VAL_44:.*]] = load i32, ptr %[[VAL_29]], align 4
// CHECK-GCN:     %[[VAL_44:.*]] = load i32, ptr addrspace(5) %[[VAL_29]], align 4
// CHECK:         store atomic i32 %[[VAL_44]], ptr %[[VAL_45:.*]] unordered, align 4
// CHECK:         br label %[[VAL_39]]
// CHECK:       entry:
// CHECK:         %[[VAL_46:.*]] = alloca i32, align 4
// CHECK:         %[[VAL_47:.*]] = alloca i32, align 4
// CHECK:         %[[VAL_48:.*]] = alloca i32, align 4
// CHECK-PTX:     %[[VAL_49:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x
// CHECK-GCN:     %[[VAL_49:.*]] = call i32 @llvm.amdgcn.workgroup.id.x
// CHECK-PTX:     %[[VAL_50:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x
// CHECK-GCN:     %[[VAL_50:.*]] = call i32 @llvm.amdgcn.workitem.id.x
// CHECK:         %[[VAL_51:.*]] = mul nuw nsw i32 %[[VAL_49]], 6
// CHECK:         %[[VAL_52:.*]] = add nuw nsw i32 %[[VAL_51]], %[[VAL_50]]
// CHECK:         %[[VAL_53:.*]] = icmp ult i32 %[[VAL_52]], 6
// CHECK:         call void @llvm.assume(i1 %[[VAL_53]])
// CHECK:         %[[VAL_54:.*]] = add nuw nsw i32 %[[VAL_52]], 0
// CHECK:         %[[VAL_55:.*]] = udiv i32 %[[VAL_54]], 1
// CHECK:         %[[VAL_56:.*]] = urem i32 %[[VAL_55]], 3
// CHECK:         %[[VAL_57:.*]] = udiv i32 %[[VAL_54]], 3
// CHECK:         %[[VAL_58:.*]] = icmp ult i32 %[[VAL_52]], 6
// CHECK:         br i1 %[[VAL_58]], label %[[VAL_59:.*]], label %[[VAL_60:.*]]
// CHECK:       scatter_TensorFlowScatter_Mul.in_bounds-after:    ; preds = %[[VAL_61:.*]], %[[VAL_62:.*]]
// CHECK:         ret void
// CHECK:       scatter_TensorFlowScatter_Mul.in_bounds-true:     ; preds = %[[VAL_62]]
// CHECK:         %[[VAL_63:.*]] = getelementptr inbounds [2 x i32], ptr %[[VAL_64:.*]], i32 0, i32 %[[VAL_57]]
// CHECK:         %[[VAL_65:.*]] = load i32, ptr %[[VAL_63]], align 4, !invariant.load
// CHECK:         %[[VAL_66:.*]] = add i32 0, %[[VAL_65]]
// CHECK:         %[[VAL_67:.*]] = icmp ult i32 %[[VAL_65]], 3
// CHECK:         %[[VAL_68:.*]] = and i1 true, %[[VAL_67]]
// CHECK:         br i1 %[[VAL_68]], label %[[VAL_69:.*]], label %[[VAL_61]]
// CHECK:       scatter.in_bounds-after:                          ; preds = %[[VAL_70:.*]], %[[VAL_59]]
// CHECK:         br label %[[VAL_60]]
// CHECK:       scatter.in_bounds-true:                           ; preds = %[[VAL_59]]
// CHECK:         %[[VAL_71:.*]] = getelementptr inbounds [3 x [3 x i32]], ptr %[[VAL_72:.*]], i32 0, i32 %[[VAL_66]], i32 %[[VAL_56]]
// CHECK:         %[[VAL_73:.*]] = getelementptr i32, ptr %[[VAL_74:.*]], i32 %[[VAL_52]]
// CHECK:         %[[VAL_75:.*]] = getelementptr inbounds i32, ptr %[[VAL_73]], i32 0
// CHECK:         %[[VAL_76:.*]] = load i32, ptr %[[VAL_75]], align 4, !invariant.load
// CHECK-PTX:     store i32 %[[VAL_76]], ptr %[[VAL_48]], align 4
// CHECK-GCN:     store i32 %[[VAL_76]], ptr addrspace(5) %[[VAL_48]], align 4
// CHECK-PTX:     %[[VAL_77:.*]] = load i32, ptr %[[VAL_48]], align 4
// CHECK-GCN:     %[[VAL_77:.*]] = load i32, ptr addrspace(5) %[[VAL_48]], align 4
// CHECK:         %[[VAL_78:.*]] = load i32, ptr %[[VAL_71]], align 4
// CHECK-PTX:     store i32 %[[VAL_78]], ptr %[[VAL_47]], align 4
// CHECK-GCN:     store i32 %[[VAL_78]], ptr addrspace(5) %[[VAL_47]], align 4
// CHECK:         br label %[[VAL_79:.*]]
// CHECK:       atomic_op_loop_exit:                              ; preds = %[[VAL_80:.*]], %[[VAL_79]]
// CHECK:         br label %[[VAL_61]]
// CHECK:       atomic_op_loop_body:                              ; preds = %[[VAL_80]], %[[VAL_69]]
// CHECK-PTX:     %[[VAL_81:.*]] = load i32, ptr %[[VAL_47]], align 4
// CHECK-GCN:     %[[VAL_81:.*]] = load i32, ptr addrspace(5) %[[VAL_47]], align 4
// CHECK-PTX:     store i32 %[[VAL_81]], ptr %[[VAL_46]], align 4
// CHECK-GCN:     store i32 %[[VAL_81]], ptr addrspace(5) %[[VAL_46]], align 4
// CHECK-GCN:     %[[VAL_46_2:.*]] = addrspacecast ptr addrspace(5) %[[VAL_46]] to ptr
// CHECK-GCN:     %[[VAL_48_2:.*]] = addrspacecast ptr addrspace(5) %[[VAL_48]] to ptr
// CHECK-GCN:     %[[VAL_46_3:.*]] = addrspacecast ptr addrspace(5) %[[VAL_46]] to ptr
// CHECK-GCN:     call void @mul_{{.*}}(ptr %[[VAL_46_2]], ptr %[[VAL_48_2]], ptr %[[VAL_46_3]])
// CHECK-PTX:     call void @mul_{{.*}}(ptr %[[VAL_46]], ptr %[[VAL_48]], ptr %[[VAL_46]])
// CHECK-PTX:     %[[VAL_82:.*]] = load i32, ptr %[[VAL_46]], align 4
// CHECK-GCN:     %[[VAL_82:.*]] = load i32, ptr addrspace(5) %[[VAL_46]], align 4
// CHECK:         %[[VAL_83:.*]] = icmp eq i32 %[[VAL_81]], %[[VAL_82]]
// CHECK:         br i1 %[[VAL_83]], label %[[VAL_70]], label %[[VAL_80]]
// CHECK:       atomic_op_loop_cas:                               ; preds = %[[VAL_79]]
// CHECK-PTX:     %[[VAL_84:.*]] = cmpxchg ptr %[[VAL_71]], i32 %[[VAL_81]], i32 %[[VAL_82]] seq_cst seq_cst, align 4
// CHECK-GCN:     %[[VAL_84:.*]] = cmpxchg ptr %[[VAL_71]], i32 %[[VAL_81]], i32 %[[VAL_82]] {{.*}} seq_cst seq_cst, align 4
// CHECK:         %[[VAL_85:.*]] = extractvalue { i32, i1 } %[[VAL_84]], 0
// CHECK-PTX:     store i32 %[[VAL_85]], ptr %[[VAL_47]], align 4
// CHECK-GCN:     store i32 %[[VAL_85]], ptr addrspace(5) %[[VAL_47]], align 4
// CHECK:         %[[VAL_86:.*]] = extractvalue { i32, i1 } %[[VAL_84]], 1
// CHECK:         br i1 %[[VAL_86]], label %[[VAL_70]], label %[[VAL_79]]
// CHECK:       entry:
// CHECK:         %[[VAL_87:.*]] = alloca i32, align 4
// CHECK:         %[[VAL_88:.*]] = load i32, ptr %[[VAL_89:.*]], align 4
// CHECK:         %[[VAL_90:.*]] = load i32, ptr %[[VAL_91:.*]], align 4
// CHECK:         %[[VAL_92:.*]] = mul i32 %[[VAL_88]], %[[VAL_90]]
// CHECK-PTX:     store i32 %[[VAL_92]], ptr %[[VAL_87]], align 4
// CHECK-GCN:     store i32 %[[VAL_92]], ptr addrspace(5) %[[VAL_87]], align 4
// CHECK-PTX:     %[[VAL_93:.*]] = load i32, ptr %[[VAL_87]], align 4
// CHECK-GCN:     %[[VAL_93:.*]] = load i32, ptr addrspace(5) %[[VAL_87]], align 4
// CHECK:         store i32 %[[VAL_93]], ptr %[[VAL_94:.*]], align 4
// CHECK:         ret void
// CHECK:       entry:
// CHECK:         %[[VAL_95:.*]] = alloca i32, align 4
// CHECK-PTX:     %[[VAL_96:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x
// CHECK-GCN:     %[[VAL_96:.*]] = call i32 @llvm.amdgcn.workgroup.id.x
// CHECK-PTX:     %[[VAL_97:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x
// CHECK-GCN:     %[[VAL_97:.*]] = call i32 @llvm.amdgcn.workitem.id.x
// CHECK:         %[[VAL_98:.*]] = mul nuw nsw i32 %[[VAL_96]], 1
// CHECK:         %[[VAL_99:.*]] = add nuw nsw i32 %[[VAL_98]], %[[VAL_97]]
// CHECK:         %[[VAL_100:.*]] = icmp ult i32 %[[VAL_99]], 1
// CHECK:         call void @llvm.assume(i1 %[[VAL_100]])
// CHECK:         %[[VAL_101:.*]] = add nuw nsw i32 %[[VAL_99]], 0
// CHECK:         %[[VAL_102:.*]] = icmp ult i32 %[[VAL_99]], 1
// CHECK:         br i1 %[[VAL_102]], label %[[VAL_103:.*]], label %[[VAL_104:.*]]
// CHECK:       scatter_ScalarUpdate.in_bounds-after:             ; preds = %[[VAL_105:.*]], %[[VAL_106:.*]]
// CHECK:         ret void
// CHECK:       scatter_ScalarUpdate.in_bounds-true:              ; preds = %[[VAL_106]]
// CHECK:         %[[VAL_107:.*]] = load i32, ptr %[[VAL_108:.*]], align 4, !invariant.load
// CHECK:         %[[VAL_109:.*]] = add i32 0, %[[VAL_107]]
// CHECK:         %[[VAL_110:.*]] = icmp ult i32 %[[VAL_107]], 4
// CHECK:         %[[VAL_111:.*]] = and i1 true, %[[VAL_110]]
// CHECK:         br i1 %[[VAL_111]], label %[[VAL_112:.*]], label %[[VAL_105]]
// CHECK:       scatter.in_bounds-after:                          ; preds = %[[VAL_112]], %[[VAL_103]]
// CHECK:         br label %[[VAL_104]]
// CHECK:       scatter.in_bounds-true:                           ; preds = %[[VAL_103]]
// CHECK:         %[[VAL_113:.*]] = getelementptr inbounds [4 x i32], ptr %[[VAL_114:.*]], i32 0, i32 %[[VAL_109]]
// CHECK:         %[[VAL_115:.*]] = load i32, ptr %[[VAL_116:.*]], align 4, !invariant.load
// CHECK-PTX:     store i32 %[[VAL_115]], ptr %[[VAL_95]], align 4
// CHECK-GCN:     store i32 %[[VAL_115]], ptr addrspace(5) %[[VAL_95]], align 4
// CHECK-PTX:     %[[VAL_117:.*]] = load i32, ptr %[[VAL_95]], align 4
// CHECK-GCN:     %[[VAL_117:.*]] = load i32, ptr addrspace(5) %[[VAL_95]], align 4
// CHECK:         store atomic i32 %[[VAL_117]], ptr %[[VAL_113]] unordered, align 4
// CHECK:         br label %[[VAL_105]]

HloModule TensorFlowScatterV1, is_scheduled=true

update_s32 (lhs: s32[], rhs: s32[]) -> s32[] {
  lhs = s32[] parameter(0)
  ROOT rhs = s32[] parameter(1)
}

fused_computation {
  operand = s32[3,3] parameter(0)
  indices = s32[2] parameter(1)
  updates = s32[2,3] parameter(2)
  ROOT scatter_TensorFlowScatterV1 = s32[3,3] scatter(operand, indices, updates),
      to_apply=update_s32,
      update_window_dims={1},
      inserted_window_dims={0},
      scatter_dims_to_operand_dims={0},
      index_vector_dim=1
}

ENTRY main {
  p0 = s32[3,3] parameter(0)
  p1 = s32[2] parameter(1)
  p2 = s32[2,3] parameter(2)
  ROOT wrapped_scatter = s32[3,3] fusion(p0, p1, p2), kind=kInput, calls=fused_computation
}


// -----


HloModule ScatterIntoScalar, is_scheduled=true

update_s32 {
  lhs = s32[] parameter(0)
  ROOT rhs = s32[] parameter(1)
}

fused_computation {
  parameter.1 = s32[] parameter(0)
  parameter.2 = s32[0]{0} parameter(1)
  parameter.3 = s32[] parameter(2)
  ROOT scatter_ScatterIntoScalar = s32[] scatter(parameter.1, parameter.2, parameter.3),
      update_window_dims={},
      inserted_window_dims={},
      scatter_dims_to_operand_dims={},
      index_vector_dim=0,
      to_apply=update_s32
}

ENTRY main {
  p0 = s32[] parameter(0)
  p1 = s32[0]{0} parameter(1)
  p2 = s32[] parameter(2)
  ROOT wrapped_scatter = s32[] fusion(p0, p1, p2), kind=kInput, calls=fused_computation
}


// -----


HloModule TensorFlowScatter_Mul, is_scheduled=true

mul_s32 (lhs: s32[], rhs: s32[]) -> s32[] {
  lhs = s32[] parameter(0)
  rhs = s32[] parameter(1)
  ROOT mul = s32[] multiply(s32[] lhs, s32[] rhs)
}

fused_computation {
  operand = s32[3,3] parameter(0)
  indices = s32[2] parameter(1)
  updates = s32[2,3] parameter(2)
  ROOT scatter_TensorFlowScatter_Mul = s32[3,3] scatter(operand, indices, updates),
      to_apply=mul_s32,
      update_window_dims={1},
      inserted_window_dims={0},
      scatter_dims_to_operand_dims={0},
      index_vector_dim=1
}

ENTRY main {
  p0 = s32[3,3] parameter(0)
  p1 = s32[2] parameter(1)
  p2 = s32[2,3] parameter(2)
  ROOT wrapped_scatter = s32[3,3] fusion(p0, p1, p2), kind=kInput, calls=fused_computation
}

// -----


HloModule ScalarUpdate, is_scheduled=true

update_s32 (lhs: s32[], rhs: s32[]) -> s32[] {
  lhs = s32[] parameter(0)
  ROOT rhs = s32[] parameter(1)
}

fused_computation {
  operand = s32[4]{0} parameter(0)
  index = s32[] parameter(1)
  updates = s32[] parameter(2)
  ROOT scatter_ScalarUpdate = s32[4]{0} scatter(operand, index, updates),
      to_apply=update_s32,
      update_window_dims={},
      inserted_window_dims={0},
      scatter_dims_to_operand_dims={0},
      index_vector_dim=0
}

ENTRY main {
  p0 = s32[4]{0} parameter(0)
  p1 = s32[] parameter(1)
  p2 = s32[] parameter(2)
  ROOT wrapped_scatter = s32[4] fusion(p0, p1, p2), kind=kInput, calls=fused_computation
}

// -----


HloModule TensorFlowScatter_Add, is_scheduled=true

add_f16 (lhs: f16[], rhs: f16[]) -> f16[] {
  lhs = f16[] parameter(0)
  rhs = f16[] parameter(1)
  ROOT add = f16[] add(f16[] lhs, f16[] rhs)
}

fused_computation {
  operand = f16[3,3] parameter(0)
  indices = s32[2] parameter(1)
  updates = f16[2,3] parameter(2)
  ROOT scatter_TensorFlowScatter_Mul = f16[3,3] scatter(operand, indices, updates),
      to_apply=add_f16,
      update_window_dims={1},
      inserted_window_dims={0},
      scatter_dims_to_operand_dims={0},
      index_vector_dim=1
}

ENTRY main {
  p0 = f16[3,3] parameter(0)
  p1 = s32[2] parameter(1)
  p2 = f16[2,3] parameter(2)
  ROOT wrapped_scatter = f16[3,3] fusion(p0, p1, p2), kind=kInput, calls=fused_computation
}

// CHECK-PTX: atomicrmw fadd
