diff --git a/lib/Dialect/TritonGPU/Transforms/AccelerateMatmul.cpp b/lib/Dialect/TritonGPU/Transforms/AccelerateMatmul.cpp
--- a/lib/Dialect/TritonGPU/Transforms/AccelerateMatmul.cpp
+++ b/lib/Dialect/TritonGPU/Transforms/AccelerateMatmul.cpp
@@ -365,7 +365,8 @@ class SparseBlockedToMMA : public mlir::
 
     assert(computeCapability >= 80 &&
            "SparseDot is supported on Ampere and higher");
-    int versionMajor = computeCapability < 90 ? 2 : 3;
+    bool allowV3 = triton::tools::getBoolEnv("ENABLE_MMA_V3");
+    int versionMajor = computeCapability >= 90 && allowV3 ? 3 : 2;
 
     // get MMA encoding for the given number of warps
     auto retShapePerCTA = ttg::getShapePerCTA(oldRetType);
diff --git a/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/ConvertLayoutOpToLLVM/SharedToSparseDotOperand.cpp b/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/ConvertLayoutOpToLLVM/SharedToSparseDotOperand.cpp
--- a/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/ConvertLayoutOpToLLVM/SharedToSparseDotOperand.cpp
+++ b/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/ConvertLayoutOpToLLVM/SharedToSparseDotOperand.cpp
@@ -31,7 +31,13 @@ Value convertLayout(
   // Calculate offset in the tile for the current thread.
   Value threadsPerWarp = i32_val(kThreadsPerWarp);
   Value warpId = udiv(thread, threadsPerWarp);
-  Value warpGroupId = udiv(warpId, i32_val(warpsPerCTA[1]));
+  Value warpGroupId;
+  if (mmaLayout.isHopper()) {
+    warpGroupId = urem(warpId, i32_val(warpsPerCTA[0]));
+  } else {
+    assert(mmaLayout.isAmpere());
+    warpGroupId = udiv(warpId, i32_val(warpsPerCTA[1]));
+  }
   Value laneId = urem(thread, threadsPerWarp);
   Value laneGroupId = udiv(laneId, i32_val(kThreadsInGroup));
   Value columnId = urem(laneId, i32_val(shapePerCTATile[1]));
